{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNMaaioXFDqF1jEnFbj1Pn9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# 1. Install required package\n","!pip install pillow-heif"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NECFihAoGfzs","executionInfo":{"status":"ok","timestamp":1740873871963,"user_tz":480,"elapsed":2616,"user":{"displayName":"Erubiel Ramos","userId":"09437047253836357023"}},"outputId":"be4eb677-df87-47ad-8062-d8290f53d183"},"execution_count":57,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pillow-heif in /usr/local/lib/python3.11/dist-packages (0.21.0)\n","Requirement already satisfied: pillow>=10.1.0 in /usr/local/lib/python3.11/dist-packages (from pillow-heif) (11.1.0)\n"]}]},{"cell_type":"code","source":["# 2. Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JYfEchrdGnLm","executionInfo":{"status":"ok","timestamp":1740873875037,"user_tz":480,"elapsed":592,"user":{"displayName":"Erubiel Ramos","userId":"09437047253836357023"}},"outputId":"11940a51-af27-44b0-c77b-2912b833c4f7"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["!ls -la /content/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uLGaCeLZ_oFa","executionInfo":{"status":"ok","timestamp":1740873932672,"user_tz":480,"elapsed":103,"user":{"displayName":"Erubiel Ramos","userId":"09437047253836357023"}},"outputId":"ffa4387e-a06e-45cd-8891-ea559e531cae"},"execution_count":60,"outputs":[{"output_type":"stream","name":"stdout","text":["total 44\n","drwxr-xr-x 1 root root 4096 Mar  1 23:46 .\n","drwxr-xr-x 1 root root 4096 Mar  1 22:09 ..\n","-rw-r--r-- 1 root root 9980 Mar  1 22:22 banana_classification.py\n","drwxr-xr-x 4 root root 4096 Feb 27 14:21 .config\n","drwx------ 7 root root 4096 Mar  1 23:46 drive\n","drwxr-xr-x 2 root root 4096 Mar  1 23:28 __pycache__\n","-rw-r--r-- 1 root root  224 Mar  1 23:20 run_tests.py\n","drwxr-xr-x 1 root root 4096 Feb 27 14:22 sample_data\n","-rw-r--r-- 1 root root 1461 Mar  1 23:28 test_banana_classification.py\n"]}]},{"cell_type":"code","execution_count":62,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V0XfNeAswS3d","executionInfo":{"status":"ok","timestamp":1740874278984,"user_tz":480,"elapsed":169,"user":{"displayName":"Erubiel Ramos","userId":"09437047253836357023"}},"outputId":"cd6e8e2c-8c7d-4c08-eaa8-1d4b2504c66a"},"outputs":[{"output_type":"stream","name":"stderr","text":["test_confusion_matrix (__main__.BananaRipenessTests.test_confusion_matrix)\n","Test confusion matrix calculation ... ok\n","test_load_and_preprocess_image (__main__.BananaRipenessTests.test_load_and_preprocess_image)\n","Test image preprocessing function ... ok\n","test_model_inference (__main__.BananaRipenessTests.test_model_inference)\n","Test the model inference pipeline ... ok\n","test_save_results_to_csv (__main__.BananaRipenessTests.test_save_results_to_csv)\n","Test CSV saving functionality ... ok\n","\n","----------------------------------------------------------------------\n","Ran 4 tests in 0.078s\n","\n","OK\n"]},{"output_type":"stream","name":"stdout","text":["\n","=============================================\n","BANANA RIPENESS CLASSIFICATION TESTS\n","=============================================\n","\n","Test setup complete. Test directory: /tmp/tmpveg7jeof\n","\n","Running test_confusion_matrix...\n","Mock Confusion Matrix:\n","OverRipe: [0 0 0 1]\n","Ripe: [0 2 0 0]\n","Unripe: [0 0 1 0]\n","VeryRipe: [0 0 0 1]\n","✓ Confusion matrix test passed\n","Test cleanup complete.\n","Test setup complete. Test directory: /tmp/tmpxgxxhkif\n","\n","Running test_load_and_preprocess_image...\n","Mock preprocessing image: /tmp/tmpxgxxhkif/test_img.jpg\n","✓ Image preprocessing test passed\n","Test cleanup complete.\n","Test setup complete. Test directory: /tmp/tmpzscgq_ex\n","\n","Running test_model_inference...\n","Mock testing images in /tmp/tmpzscgq_ex with model /tmp/tmpzscgq_ex/mock_model.tflite\n","✓ Model inference test passed\n","Test cleanup complete.\n","Test setup complete. Test directory: /tmp/tmps2cdxpgy\n","\n","Running test_save_results_to_csv...\n","Mock saving results to /tmp/tmps2cdxpgy/test_results.csv\n","✓ CSV saving test passed\n","Test cleanup complete.\n","\n","=============================================\n","SUMMARY: 4 tests run\n","Passed: 4\n","Failed: 0\n","Errors: 0\n","=============================================\n","\n","\n","Test suite passed successfully!\n"]}],"source":["# Unit Test for TestModelRealData.ipynb\n","import unittest\n","import os\n","import numpy as np\n","import tensorflow as tf\n","from unittest.mock import patch, MagicMock\n","import tempfile\n","import shutil\n","from PIL import Image\n","import sys\n","\n","# Define global variables\n","CLASS_LABELS = ['OverRipe', 'Ripe', 'Unripe', 'VeryRipe']\n","\n","# Define mock functions for testing\n","def mock_load_and_preprocess_image(image_path, target_size=(224, 224), input_scale=1.0, input_zero_point=0):\n","    \"\"\"Mock implementation for testing\"\"\"\n","    print(f\"Mock preprocessing image: {image_path}\")\n","    img = Image.open(image_path)\n","    img = img.resize(target_size)\n","    img_array = np.array(img).astype(np.float32)\n","    processed_image = np.expand_dims(img_array, axis=0)\n","    return processed_image\n","\n","def mock_save_results_to_csv(results, output_file='classification_results.csv'):\n","    \"\"\"Mock implementation for testing\"\"\"\n","    print(f\"Mock saving results to {output_file}\")\n","    with open(output_file, 'w') as f:\n","        f.write('image,actual_class,predicted_class,confidence\\n')\n","        for result in results:\n","            f.write(f\"{result['image']},{result['actual_class']},{result['predicted_class']},{result['confidence']:.2f}%\\n\")\n","    return True\n","\n","def mock_print_confusion_matrix(results, class_labels):\n","    \"\"\"Mock implementation for testing\"\"\"\n","    print(\"Mock Confusion Matrix:\")\n","    confusion_matrix = np.zeros((len(class_labels), len(class_labels)), dtype=int)\n","    for r in results:\n","        actual_idx = class_labels.index(r['actual_class'])\n","        pred_idx = class_labels.index(r['predicted_class'])\n","        confusion_matrix[actual_idx][pred_idx] += 1\n","\n","    # Print simplified matrix\n","    for i, row in enumerate(confusion_matrix):\n","        print(f\"{class_labels[i]}: {row}\")\n","    return True\n","\n","def mock_test_real_world_images(model_path, test_dir):\n","    \"\"\"Mock implementation for testing\"\"\"\n","    print(f\"Mock testing images in {test_dir} with model {model_path}\")\n","    results = []\n","    # Create mock results for each class\n","    for cls in CLASS_LABELS:\n","        cls_dir = os.path.join(test_dir, cls)\n","        if os.path.exists(cls_dir):\n","            # Add a mock result\n","            import random\n","            result = {\n","                'image': f'test_{cls.lower()}.jpg',\n","                'actual_class': cls,\n","                'predicted_class': cls,  # Assume correct prediction\n","                'confidence': random.uniform(80, 95),\n","                'probabilities': {c: 10.0 for c in CLASS_LABELS}\n","            }\n","            # Set the predicted class probability higher\n","            result['probabilities'][cls] = 85.0\n","            results.append(result)\n","\n","    return results\n","\n","# Use our mock functions\n","load_and_preprocess_image = mock_load_and_preprocess_image\n","save_results_to_csv = mock_save_results_to_csv\n","print_confusion_matrix = mock_print_confusion_matrix\n","test_real_world_images = mock_test_real_world_images\n","\n","class BananaRipenessTests(unittest.TestCase):\n","    def setUp(self):\n","        \"\"\"Set up test environment\"\"\"\n","        # Create temporary directory for test files\n","        self.test_dir = tempfile.mkdtemp()\n","\n","        # Create test folders for each class\n","        for cls in CLASS_LABELS:\n","            os.makedirs(os.path.join(self.test_dir, cls), exist_ok=True)\n","\n","        # Create a dummy test image in each class folder\n","        for cls in CLASS_LABELS:\n","            self.create_dummy_image(os.path.join(self.test_dir, cls, f'test_{cls.lower()}.jpg'))\n","\n","        # Create a mock model file\n","        self.model_path = os.path.join(self.test_dir, 'mock_model.tflite')\n","        with open(self.model_path, 'w') as f:\n","            f.write('Mock TFLite model content')\n","\n","        print(f\"Test setup complete. Test directory: {self.test_dir}\")\n","\n","    def tearDown(self):\n","        \"\"\"Clean up after tests\"\"\"\n","        shutil.rmtree(self.test_dir)\n","        print(\"Test cleanup complete.\")\n","\n","    def create_dummy_image(self, path, size=(224, 224), color='yellow'):\n","        \"\"\"Create a dummy image for testing\"\"\"\n","        try:\n","            img = Image.new('RGB', size, color=color)\n","            img.save(path)\n","            return path\n","        except Exception as e:\n","            print(f\"Error creating dummy image: {e}\")\n","            return None\n","\n","    def test_load_and_preprocess_image(self):\n","        \"\"\"Test image preprocessing function\"\"\"\n","        print(\"\\nRunning test_load_and_preprocess_image...\")\n","        test_img_path = os.path.join(self.test_dir, 'test_img.jpg')\n","        self.create_dummy_image(test_img_path)\n","\n","        # Call the function\n","        preprocessed = load_and_preprocess_image(test_img_path)\n","\n","        # Basic validation\n","        self.assertIsNotNone(preprocessed)\n","        self.assertEqual(len(preprocessed.shape), 4)  # Should be (1, height, width, channels)\n","        self.assertEqual(preprocessed.shape[0], 1)  # Batch dimension\n","        print(\"✓ Image preprocessing test passed\")\n","\n","    def test_save_results_to_csv(self):\n","        \"\"\"Test CSV saving functionality\"\"\"\n","        print(\"\\nRunning test_save_results_to_csv...\")\n","        # Create mock results\n","        results = [{\n","            'image': 'test_banana.jpg',\n","            'actual_class': 'Ripe',\n","            'predicted_class': 'Ripe',\n","            'confidence': 95.0,\n","            'probabilities': {cls: (95.0 if cls == 'Ripe' else 5.0/(len(CLASS_LABELS)-1))\n","                             for cls in CLASS_LABELS}\n","        }]\n","\n","        output_file = os.path.join(self.test_dir, 'test_results.csv')\n","\n","        # Call function\n","        save_results_to_csv(results, output_file)\n","\n","        # Check that file exists and has content\n","        self.assertTrue(os.path.exists(output_file))\n","        with open(output_file, 'r') as f:\n","            content = f.read()\n","            self.assertIn('image', content)\n","            self.assertIn('Ripe', content)\n","        print(\"✓ CSV saving test passed\")\n","\n","    def test_confusion_matrix(self):\n","        \"\"\"Test confusion matrix calculation\"\"\"\n","        print(\"\\nRunning test_confusion_matrix...\")\n","        # Mock classification results with various classification outcomes\n","        results = [\n","            {'actual_class': 'Ripe', 'predicted_class': 'Ripe'},\n","            {'actual_class': 'Ripe', 'predicted_class': 'Ripe'},\n","            {'actual_class': 'Unripe', 'predicted_class': 'Unripe'},\n","            {'actual_class': 'OverRipe', 'predicted_class': 'VeryRipe'},  # Misclassification\n","            {'actual_class': 'VeryRipe', 'predicted_class': 'VeryRipe'}\n","        ]\n","\n","        # Call function\n","        print_confusion_matrix(results, CLASS_LABELS)\n","\n","        # If we get here without exception, the test passes\n","        print(\"✓ Confusion matrix test passed\")\n","\n","    def test_model_inference(self):\n","        \"\"\"Test the model inference pipeline\"\"\"\n","        print(\"\\nRunning test_model_inference...\")\n","\n","        # Call the inference function with our mock implementation\n","        results = test_real_world_images(self.model_path, self.test_dir)\n","\n","        # Verify we got results back\n","        self.assertIsInstance(results, list)\n","        self.assertGreaterEqual(len(results), 1)\n","        print(\"✓ Model inference test passed\")\n","\n","def run_banana_tests():\n","    \"\"\"Run all banana ripeness classification tests\"\"\"\n","    print(\"\\n=============================================\")\n","    print(\"BANANA RIPENESS CLASSIFICATION TESTS\")\n","    print(\"=============================================\\n\")\n","\n","    # Create and run the test suite\n","    suite = unittest.TestLoader().loadTestsFromTestCase(BananaRipenessTests)\n","    result = unittest.TextTestRunner(verbosity=2).run(suite)\n","\n","    # Print summary\n","    print(\"\\n=============================================\")\n","    print(f\"SUMMARY: {result.testsRun} tests run\")\n","    print(f\"Passed: {result.testsRun - len(result.errors) - len(result.failures)}\")\n","    print(f\"Failed: {len(result.failures)}\")\n","    print(f\"Errors: {len(result.errors)}\")\n","    print(\"=============================================\\n\")\n","\n","    return len(result.errors) == 0 and len(result.failures) == 0\n","\n","# Run the tests when this script is executed\n","if __name__ == \"__main__\":\n","    success = run_banana_tests()\n","    print(f\"\\nTest suite {'passed successfully!' if success else 'failed.'}\")\n"]}]}