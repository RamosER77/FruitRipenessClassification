{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNpzy9PMFuXqrwt4FKTmB1N"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Y24P-_edn4H","executionInfo":{"status":"ok","timestamp":1743043119076,"user_tz":420,"elapsed":9512,"user":{"displayName":"Erubiel Ramos","userId":"09437047253836357023"}},"outputId":"738910cf-a504-496d-838c-0307122a315d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting imagehash\n","  Downloading ImageHash-4.3.2-py2.py3-none-any.whl.metadata (8.4 kB)\n","Collecting PyWavelets (from imagehash)\n","  Downloading pywavelets-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from imagehash) (2.0.2)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from imagehash) (11.1.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from imagehash) (1.14.1)\n","Downloading ImageHash-4.3.2-py2.py3-none-any.whl (296 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.7/296.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pywavelets-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: PyWavelets, imagehash\n","Successfully installed PyWavelets-1.8.0 imagehash-4.3.2\n"]}],"source":["!pip install imagehash"]},{"cell_type":"code","source":["import os\n","from pathlib import Path\n","import numpy as np\n","from PIL import Image\n","import imagehash\n","from collections import defaultdict\n","import pandas as pd\n","import shutil\n","\n","def check_and_rename_duplicate_images(paths, rename=True):\n","    \"\"\"\n","    Check for duplicate images across train, valid, and test folders\n","    using both filename and image content comparison, and rename duplicates if specified.\n","\n","    Args:\n","        paths (dict): Dictionary containing paths to train, valid, and test folders\n","        rename (bool): Whether to rename duplicate images or just report them\n","\n","    Returns:\n","        tuple: (filename_duplicates, content_duplicates, renamed_images)\n","            - filename_duplicates: Dictionary of duplicate filenames\n","            - content_duplicates: Dictionary of duplicate images based on content\n","            - renamed_images: List of images that were renamed\n","    \"\"\"\n","    # Store all filenames\n","    filename_map = defaultdict(list)\n","    # Store image hashes\n","    hash_map = defaultdict(list)\n","    # Track renamed images\n","    renamed_images = []\n","\n","    # Process each split (train/valid/test)\n","    for split in ['train', 'valid', 'test']:\n","        if os.path.exists(paths[split]):\n","            # Walk through all subdirectories\n","            for class_name in os.listdir(paths[split]):\n","                class_dir = os.path.join(paths[split], class_name)\n","                if os.path.isdir(class_dir):\n","                    # Process each image\n","                    for img_name in os.listdir(class_dir):\n","                        if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n","                            # Store full path and split info for filename check\n","                            filename_map[img_name].append({\n","                                'path': os.path.join(class_dir, img_name),\n","                                'split': split,\n","                                'class': class_name\n","                            })\n","\n","                            # Calculate image hash for content comparison\n","                            try:\n","                                img_path = os.path.join(class_dir, img_name)\n","                                with Image.open(img_path) as img:\n","                                    # Convert to RGB if necessary\n","                                    if img.mode != 'RGB':\n","                                        img = img.convert('RGB')\n","                                    # Calculate perceptual hash\n","                                    img_hash = str(imagehash.average_hash(img))\n","                                    hash_map[img_hash].append({\n","                                        'path': img_path,\n","                                        'split': split,\n","                                        'class': class_name,\n","                                        'filename': img_name\n","                                    })\n","                            except Exception as e:\n","                                print(f\"Error processing {img_path}: {str(e)}\")\n","\n","    # Find duplicates by filename\n","    filename_duplicates = {\n","        filename: locations\n","        for filename, locations in filename_map.items()\n","        if len(locations) > 1\n","    }\n","\n","    # Find duplicates by content\n","    content_duplicates = {\n","        hash_val: locations\n","        for hash_val, locations in hash_map.items()\n","        if len(locations) > 1\n","    }\n","\n","    # Rename duplicates if requested\n","    if rename:\n","        # Rename filename duplicates\n","        for filename, locations in filename_duplicates.items():\n","            for i, loc in enumerate(locations[1:], 1):  # Skip the first one (original)\n","                old_path = loc['path']\n","                file_base, file_ext = os.path.splitext(filename)\n","                new_filename = f\"{file_base}_tom{i}a{file_ext}\"\n","                new_path = os.path.join(os.path.dirname(old_path), new_filename)\n","\n","                # Rename file\n","                try:\n","                    shutil.move(old_path, new_path)\n","                    renamed_images.append({\n","                        'original_path': old_path,\n","                        'new_path': new_path,\n","                        'original_name': filename,\n","                        'new_name': new_filename,\n","                        'duplicate_type': 'filename'\n","                    })\n","                    print(f\"Renamed: {old_path} -> {new_path}\")\n","                except Exception as e:\n","                    print(f\"Error renaming {old_path}: {str(e)}\")\n","\n","        # Rename content duplicates (that aren't already filename duplicates)\n","        processed_paths = set([item['original_path'] for item in renamed_images])\n","\n","        for hash_val, locations in content_duplicates.items():\n","            for i, loc in enumerate(locations[1:], 1):  # Skip the first one (original)\n","                old_path = loc['path']\n","                # Skip if already renamed\n","                if old_path in processed_paths:\n","                    continue\n","\n","                file_base, file_ext = os.path.splitext(loc['filename'])\n","                new_filename = f\"{file_base}_dup{i}b{file_ext}\"\n","                new_path = os.path.join(os.path.dirname(old_path), new_filename)\n","\n","                # Rename file\n","                try:\n","                    shutil.move(old_path, new_path)\n","                    renamed_images.append({\n","                        'original_path': old_path,\n","                        'new_path': new_path,\n","                        'original_name': loc['filename'],\n","                        'new_name': new_filename,\n","                        'duplicate_type': 'content'\n","                    })\n","                    print(f\"Renamed: {old_path} -> {new_path}\")\n","                    processed_paths.add(old_path)\n","                except Exception as e:\n","                    print(f\"Error renaming {old_path}: {str(e)}\")\n","\n","    return filename_duplicates, content_duplicates, renamed_images\n","\n","def print_duplicate_summary(filename_duplicates, content_duplicates, renamed_images=None):\n","    \"\"\"Print a summary of found duplicates and renamed images\"\"\"\n","    print(\"\\n=== Duplicate Analysis Summary ===\")\n","\n","    print(\"\\nDuplicates by filename:\")\n","    if filename_duplicates:\n","        for filename, locations in filename_duplicates.items():\n","            print(f\"\\nFilename: {filename}\")\n","            for loc in locations:\n","                print(f\"- Found in {loc['split']}/{loc['class']}\")\n","    else:\n","        print(\"No duplicate filenames found.\")\n","\n","    print(\"\\nDuplicates by content:\")\n","    if content_duplicates:\n","        for hash_val, locations in content_duplicates.items():\n","            print(f\"\\nHash: {hash_val}\")\n","            for loc in locations:\n","                print(f\"- {loc['filename']} in {loc['split']}/{loc['class']}\")\n","    else:\n","        print(\"No duplicate content found.\")\n","\n","    if renamed_images:\n","        print(\"\\nRenamed Images:\")\n","        print(f\"Total renamed: {len(renamed_images)}\")\n","        for item in renamed_images[:5]:  # Show first 5 as example\n","            print(f\"- {item['original_name']} -> {item['new_name']} ({item['duplicate_type']} duplicate)\")\n","        if len(renamed_images) > 5:\n","            print(f\"... and {len(renamed_images) - 5} more\")\n","\n","def generate_duplicate_report(filename_duplicates, content_duplicates, renamed_images=None):\n","    \"\"\"Generate pandas DataFrames for detailed duplicate analysis\"\"\"\n","    # Prepare data for filename duplicates\n","    filename_data = []\n","    for filename, locations in filename_duplicates.items():\n","        for loc in locations:\n","            filename_data.append({\n","                'filename': filename,\n","                'split': loc['split'],\n","                'class': loc['class'],\n","                'full_path': loc['path']\n","            })\n","\n","    # Prepare data for content duplicates\n","    content_data = []\n","    for hash_val, locations in content_duplicates.items():\n","        for loc in locations:\n","            content_data.append({\n","                'hash': hash_val,\n","                'filename': loc['filename'],\n","                'split': loc['split'],\n","                'class': loc['class'],\n","                'full_path': loc['path']\n","            })\n","\n","    # Create DataFrames\n","    filename_df = pd.DataFrame(filename_data) if filename_data else pd.DataFrame()\n","    content_df = pd.DataFrame(content_data) if content_data else pd.DataFrame()\n","    renamed_df = pd.DataFrame(renamed_images) if renamed_images else pd.DataFrame()\n","\n","    return filename_df, content_df, renamed_df\n","\n","def main(rename_duplicates=True):\n","    # Define base directory and paths\n","    base_dir = Path('/content/drive/MyDrive/SeniorProject/Tomato/RipenessClassification_Sorted')\n","    paths = {\n","        'base': base_dir,\n","        'valid': os.path.join(str(base_dir), 'valid'),\n","        'train': os.path.join(str(base_dir), 'train'),\n","        'test': os.path.join(str(base_dir), 'test')\n","    }\n","\n","    # Check for duplicates and rename if requested\n","    print(f\"Checking for duplicate images{' and renaming them' if rename_duplicates else ''}...\")\n","    filename_duplicates, content_duplicates, renamed_images = check_and_rename_duplicate_images(\n","        paths, rename=rename_duplicates\n","    )\n","\n","    # Print summary\n","    print_duplicate_summary(filename_duplicates, content_duplicates, renamed_images)\n","\n","    # Generate detailed report\n","    filename_df, content_df, renamed_df = generate_duplicate_report(\n","        filename_duplicates, content_duplicates, renamed_images\n","    )\n","\n","    # Save reports if duplicates were found\n","    if not filename_df.empty:\n","        filename_df.to_csv('duplicate_filenames_report.csv', index=False)\n","        print(\"\\nDuplicate filenames report saved to 'duplicate_filenames_report.csv'\")\n","\n","    if not content_df.empty:\n","        content_df.to_csv('duplicate_content_report.csv', index=False)\n","        print(\"\\nDuplicate content report saved to 'duplicate_content_report.csv'\")\n","\n","    if not renamed_df.empty:\n","        renamed_df.to_csv('renamed_images_report.csv', index=False)\n","        print(\"\\nRenamed images report saved to 'renamed_images_report.csv'\")\n","\n","    return filename_df, content_df, renamed_df\n","\n","if __name__ == \"__main__\":\n","    # Set rename_duplicates=False if you only want to detect without renaming\n","    filename_df, content_df, renamed_df = main(rename_duplicates=True)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kTHF2Lmgd1ve","executionInfo":{"status":"ok","timestamp":1743043371710,"user_tz":420,"elapsed":34,"user":{"displayName":"Erubiel Ramos","userId":"09437047253836357023"}},"outputId":"e1f51556-9f9c-4e6a-811b-a88ac1d34b87"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Checking for duplicate images and renaming them...\n","\n","=== Duplicate Analysis Summary ===\n","\n","Duplicates by filename:\n","No duplicate filenames found.\n","\n","Duplicates by content:\n","No duplicate content found.\n"]}]}]}